# analysis/metadata_viewer.py
# -*- coding: utf-8 -*-
import dotenv
dotenv.load_dotenv(override=True)

import os
import io
import json
import math
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional
import pandas as pd

import datasets
import streamlit as st
from PIL import Image


# ============================ Page Config ============================
st.set_page_config(page_title="Med-VLM Metadata Viewer", layout="wide")


# ============================ Helpers ============================

def _normalize_list_str(x) -> List[str]:
    """ensure list[str]"""
    if x is None:
        return []
    if isinstance(x, list):
        return [str(v) for v in x]
    return [str(x)]

def _safe_get_list(row: Dict[str, Any], key: str) -> List[Any]:
    v = row.get(key, [])
    return v if isinstance(v, list) else []

def _safe_index(lst: List[Any], idx: int, default=None):
    try:
        return lst[idx]
    except Exception:
        return default

def make_composite_key(rec: Dict[str, Any]) -> Tuple:
    """
    composite key: article_accession_id + tuple(image_id) + tuple(image_file_name)
    both (Parquet/JSONL) follow this rule
    """
    aid = rec.get("article_accession_id", None)
    img_ids = tuple(_normalize_list_str(rec.get("image_id", [])))
    img_names = tuple(_normalize_list_str(rec.get("image_file_name", [])))
    return (aid, img_ids, img_names)


def to_pil_from_parquet_image_entry(entry: Any) -> Optional[Image.Image]:
    try:
        if isinstance(entry, dict):
            b = entry.get("bytes", None)
            if b is not None:
                if isinstance(b, memoryview):
                    b = b.tobytes()
                elif not isinstance(b, (bytes, bytearray)):
                    b = bytes(b)
                return Image.open(io.BytesIO(b)).convert("RGB")
            p = entry.get("path", None)
            if isinstance(p, str) and os.path.exists(p):
                return Image.open(p).convert("RGB")
        if isinstance(entry, str) and os.path.exists(entry):
            return Image.open(entry).convert("RGB")
    except Exception:
        return None
    return None


def prettify_json(obj: Any, drop_keys: Optional[List[str]] = None) -> str:
    if drop_keys and isinstance(obj, dict):
        obj = {k: v for k, v in obj.items() if k not in set(drop_keys)}
    try:
        return json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        return str(obj)


def jsonl_signature(path: str, num_lines_hint: int) -> Tuple[int, int, int]:
    p = Path(path)
    try:
        st_ = p.stat()
        return (st_.st_size, int(st_.st_mtime), int(num_lines_hint))
    except Exception:
        return (0, 0, int(num_lines_hint))


# ============================ Loaders (cached) ============================

@st.cache_data(show_spinner=False)
def load_parquet_split(parquet_path_or_glob: str) -> datasets.Dataset:
    p = Path(parquet_path_or_glob)
    files: List[str] = []
    if p.exists() and p.is_dir():
        files = sorted(str(pp) for pp in p.glob("*.parquet"))
    else:
        from glob import glob
        files = sorted(glob(parquet_path_or_glob))

    if not files:
        raise FileNotFoundError(f"No parquet files matched: {parquet_path_or_glob}")

    ds_dict = datasets.load_dataset("parquet", data_files={"data": files})
    return ds_dict["data"]


@st.cache_data(show_spinner=False)
def load_jsonl_records(jsonl_path: str) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    p = Path(jsonl_path)
    if not p.exists():
        raise FileNotFoundError(f"JSONL not found: {jsonl_path}")
    with p.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                out.append(json.loads(line))
            except Exception:
                continue
    return out


@st.cache_data(show_spinner=False)
def build_indices(
    _parquet_ds: datasets.Dataset,      
    _jsonl_recs: List[Dict[str, Any]],  
    parquet_fp: str,                    
    jsonl_sig: Tuple[int, int, int], 
):
    """
    index: 
      - parquet_key2idx: composite key -> parquet line number
      - jsonl_key2rec:   composite key -> JSONL record
      - jsonl_idx2rec:   dataset_index -> JSONL record
      - matched_keys:    intersection
    """
    parquet_ds = _parquet_ds
    jsonl_recs = _jsonl_recs

    parquet_key2idx: Dict[Tuple, int] = {}
    for i in range(len(parquet_ds)):
        row = parquet_ds[i]
        key = make_composite_key(row)
        if key not in parquet_key2idx: 
            parquet_key2idx[key] = i

    jsonl_key2rec: Dict[Tuple, Dict[str, Any]] = {}
    jsonl_idx2rec: Dict[int, Dict[str, Any]] = {}
    for rec in jsonl_recs:
        key = make_composite_key(rec)
        if key not in jsonl_key2rec:
            jsonl_key2rec[key] = rec
        ds_idx = rec.get("dataset_index", None)
        if isinstance(ds_idx, int) and ds_idx not in jsonl_idx2rec:
            jsonl_idx2rec[ds_idx] = rec

    matched_keys = sorted(set(parquet_key2idx.keys()) & set(jsonl_key2rec.keys()),
                          key=lambda k: parquet_key2idx[k])

    return parquet_key2idx, jsonl_key2rec, jsonl_idx2rec, matched_keys


# ============================ Sidebar: Inputs ============================

st.sidebar.header("âš™ï¸ æ•°æ®æºå‚æ•°")

parquet_input = st.sidebar.text_input(
    "Parquet è·¯å¾„/é€šé…ç¬¦/ç›®å½•",
    "/opt/dlami/nvme/nwang60/datasets/MVT-synthesis/biomedica_webdataset_glm_VQA_parquet_25k/*.parquet",
)

jsonl_input = st.sidebar.text_input(
    "JSONLï¼ˆrubric/éªŒè¯å…ƒæ•°æ®ï¼‰è·¯å¾„",
    "/opt/dlami/nvme/nwang60/datasets/MVT-synthesis/biomedica_webdataset_glm_generated_qwen_verified_VQA_json_25k/biomedica_webdataset_glm_generated_qwen_verified_VQA_json_25k_score_filtered.jsonl",
)

# jsonl_input = st.sidebar.text_input(
#     "JSONLï¼ˆrubric/éªŒè¯å…ƒæ•°æ®ï¼‰è·¯å¾„",
#     "/opt/dlami/nvme/nwang60/datasets/MVT-synthesis/biomedica_webdataset_glm_generated_qwen_verified_VQA_json_25k/biomedica_webdataset_glm_generated_qwen_verified_VQA_json_25k_filtered_mean9670_13k_rebalanced_subset10k.jsonl",
# )


# æ§ä»¶ï¼šæŒ‰ article_accession_id è¿‡æ»¤ã€åˆ†é¡µã€è·³è½¬ dataset_index
st.sidebar.header("ğŸ” è¿‡æ»¤ä¸è·³è½¬")
filter_aid = st.sidebar.text_input("æŒ‰ article_accession_id ç²¾ç¡®è¿‡æ»¤ï¼ˆå¯ç•™ç©ºï¼‰", value="")
rows_per_page = st.sidebar.slider("æ¯é¡µæ¡æ•°", min_value=1, max_value=40, value=8, step=1)
goto_ds_index = st.sidebar.text_input("è·³è½¬ dataset_indexï¼ˆæ¥è‡ª JSONLï¼‰", value="")
go_button = st.sidebar.button("ğŸš€ åŠ è½½ / åˆ·æ–°")

# â€”â€” é»˜è®¤çŠ¶æ€ï¼Œé¿å… KeyError â€”â€” #
for k, v in [
    ("parquet_ds", None),
    ("jsonl_recs", []),
    ("parquet_key2idx", {}),
    ("jsonl_key2rec", {}),
    ("jsonl_idx2rec", {}),
    ("matched_keys", []),
    ("filtered_keys", []),
]:
    if k not in st.session_state:
        st.session_state[k] = v

# é¦–æ¬¡è‡ªåŠ¨åŠ è½½ï¼ˆå½“è·¯å¾„å­˜åœ¨æ—¶ï¼‰
auto_boot = ("_boot" not in st.session_state) and Path(parquet_input).parent.exists()
if auto_boot:
    st.session_state["_boot"] = True
    go_button = True

if go_button:
    with st.spinner("Loading parquet & jsonl ..."):
        try:
            st.session_state.parquet_ds = load_parquet_split(parquet_input)
            st.session_state.jsonl_recs = load_jsonl_records(jsonl_input)

            # å¯å“ˆå¸ŒæŒ‡çº¹
            parquet_fp = getattr(st.session_state.parquet_ds, "_fingerprint", "") or ""
            jsonl_sig = jsonl_signature(jsonl_input, num_lines_hint=len(st.session_state.jsonl_recs))

            (st.session_state.parquet_key2idx,
             st.session_state.jsonl_key2rec,
             st.session_state.jsonl_idx2rec,
             st.session_state.matched_keys) = build_indices(
                 st.session_state.parquet_ds,
                 st.session_state.jsonl_recs,
                 parquet_fp,
                 jsonl_sig,
             )
            # è¿‡æ»¤
            if filter_aid:
                st.session_state.filtered_keys = [
                    k for k in st.session_state.matched_keys if (k[0] == filter_aid)
                ]
            else:
                st.session_state.filtered_keys = list(st.session_state.matched_keys)

        except Exception as e:
            st.error(f"åŠ è½½å¤±è´¥ï¼š{e}")
            st.session_state.matched_keys = []
            st.session_state.filtered_keys = []

# Early exit
if st.session_state.parquet_ds is None:
    st.info("â¬…ï¸ åœ¨å·¦ä¾§å¡«å…¥ Parquet ä¸ JSONL è·¯å¾„åç‚¹å‡»ã€åŠ è½½ / åˆ·æ–°ã€ã€‚")
    st.stop()


# ====== Status metrics ======
with st.sidebar.expander("çŠ¶æ€ / ç»Ÿè®¡", expanded=True):
    try:
        st.markdown(f"- Parquet æ¡æ•°ï¼š`{len(st.session_state.parquet_ds)}`")
    except Exception:
        st.markdown(f"- Parquet æ¡æ•°ï¼š`?`")
    st.markdown(f"- JSONL æ¡æ•°ï¼š`{len(st.session_state.jsonl_recs)}`")
    st.markdown(f"- å¤åˆé”®é…å¯¹æ¡æ•°ï¼š`{len(st.session_state.matched_keys)}`")
    if filter_aid:
        st.markdown(f"- è¿‡æ»¤åæ¡æ•°ï¼š`{len(st.session_state.filtered_keys)}`")


# ============================ Rendering ============================

def render_pair_by_key(key: Tuple):
    """æŒ‰å¤åˆé”®æ¸²æŸ“ä¸€æ¡é…å¯¹è®°å½•ï¼ˆé€å›¾å±•ç¤º caption/context å¯¹é½ï¼‰ã€‚"""
    parquet_idx = st.session_state.parquet_key2idx.get(key, None)
    jsonl_rec = st.session_state.jsonl_key2rec.get(key, None)
    if parquet_idx is None or jsonl_rec is None:
        st.warning("æœªæ‰¾åˆ°å®Œæ•´é…å¯¹è®°å½•ã€‚")
        return

    row = st.session_state.parquet_ds[parquet_idx]

    # é¡¶éƒ¨æ¦‚è§ˆ
    st.markdown(
        f"#### Pair â€¢ Parquet idx: `{parquet_idx}` â€¢ "
        f"article_accession_id: `{key[0]}`"
    )
    st.markdown(f"**Article title:** {row.get('article_title', '')}")

    # é€å›¾å¯¹é½å±•ç¤º
    imgs = _safe_get_list(row, "images")
    caps = _safe_get_list(row, "caption")                  # list[str]
    ctxs = _safe_get_list(row, "context")                  # list[list[str]]
    img_ids = _normalize_list_str(row.get("image_id", []))
    img_names = _normalize_list_str(row.get("image_file_name", []))

    if imgs:
        st.markdown(f"**Images:** {len(imgs)} å¼ ")
        for i, ent in enumerate(imgs):
            with st.container():
                ic, tc = st.columns([1, 2])

                with ic:
                    pil = to_pil_from_parquet_image_entry(ent)
                    if pil is not None:
                        st.image(pil, use_container_width=True)
                    else:
                        st.caption("ï¼ˆæ— æ³•æ¸²æŸ“è¯¥å›¾ç‰‡ï¼‰")

                with tc:
                    this_img_id = _safe_index(img_ids, i, "<none>")
                    this_img_name = _safe_index(img_names, i, "<none>")
                    this_cap = _safe_index(caps, i, "<none>")
                    raw_ctx = _safe_index(ctxs, i, [])
                    this_ctx_list = raw_ctx if isinstance(raw_ctx, list) else [str(raw_ctx)] if raw_ctx is not None else []

                    st.markdown(f"**Image #{i+1}** â€” image_id: `{this_img_id}` â€¢ file: `{this_img_name}`")
                    st.markdown(f"**Caption:** {this_cap if this_cap is not None else '<none>'}")

                    if this_ctx_list:
                        with st.expander("Contextï¼ˆå±•å¼€æŸ¥çœ‹ï¼‰", expanded=False):
                            for j, para in enumerate(this_ctx_list, 1):
                                st.markdown(f"{j}. {para}")
                    else:
                        st.markdown("**Context:** <none>")

            st.divider()
    else:
        st.caption("æ— å›¾ç‰‡")

    # JSONL çš„ MCQï¼ˆè‹¥æœ‰ï¼‰
    gen_vqa = jsonl_rec.get("generated_vqa") or row.get("generated_vqa") or {}
    if gen_vqa:
        st.markdown("### Generated VQAï¼ˆfrom JSONL if availableï¼‰")
        q = gen_vqa.get("question", "")
        st.markdown(f"- **Q:** {q}")
        opts = gen_vqa.get("options", {})
        if isinstance(opts, dict) and opts:
            st.markdown("- **Options:**")
            for k in ["A", "B", "C", "D", "E"]:
                if k in opts:
                    st.markdown(f"  - **{k}.** {opts[k]}")
        ans = gen_vqa.get("answer", None)
        if ans is not None:
            st.markdown(f"- **Answer:** **{ans}**")

    # JSONL æ‰©å±•å…ƒæ•°æ®ï¼ˆé™¤ raw_model_output å¤–å…¨éƒ¨å±•ç¤ºï¼‰
    st.markdown("---")
    st.markdown("**JSONL æ‰©å±•å…ƒæ•°æ®**ï¼ˆå·²éšè— raw_model_outputï¼‰")

    ds_idx = jsonl_rec.get("dataset_index", None)
    st.markdown(f"- **dataset_index**: `{ds_idx}`")

    # verify_pass ç»¿è‰²/çº¢è‰²é«˜äº®
    vp = jsonl_rec.get("verify_pass", None)
    vs = jsonl_rec.get("verify_score", None)
    try:
        # æ–°ç‰ˆ Streamlit æ”¯æŒ :green[] / :red[] è¯­æ³•
        if isinstance(vp, bool):
            st.markdown(f"- **verify_pass:** {':green[True]' if vp else ':red[False]'}")
        elif vp is not None:
            st.markdown(f"- **verify_pass:** `{vp}`")
    except Exception:
        # æ—§ç‰ˆå›é€€
        if isinstance(vp, bool):
            (st.success if vp else st.error)(f"verify_pass: {vp}")
        elif vp is not None:
            st.markdown(f"- **verify_pass:** `{vp}`")

    if vs is not None:
        st.markdown(f"- **verify_score:** `{vs}`")

    # Rubric è¡¨æ ¼ï¼ˆDataFrame æ¸²æŸ“ï¼‰
    rub = (jsonl_rec.get("rubric_verification") or {}).get("rubric", None)
    if isinstance(rub, list) and rub:
        st.markdown("**Rubric æ˜ç»†**")
        rub_rows = []
        for r in rub:
            rub_rows.append({
                "idx": r.get("idx", ""),
                "category": r.get("category", ""),
                "weight": r.get("weight", ""),
                "score": r.get("score", ""),
                "title": r.get("title", ""),
                "notes": r.get("notes", ""),
            })
        rub_df = pd.DataFrame(rub_rows, columns=["idx","category","weight","score","title","notes"])
        # Streamlit è€ç‰ˆæœ¬æ²¡æœ‰ hide_index å‚æ•°ï¼Œè¿™é‡Œç»Ÿä¸€ reset_index ä»¥å…å‡ºç°åŒç´¢å¼•
        rub_df = rub_df.reset_index(drop=True)
        st.dataframe(rub_df, use_container_width=True)

    # å…¶ä½™ JSONL å­—æ®µï¼ˆéšè— raw_model_outputï¼‰
    with st.expander("æŸ¥çœ‹ JSONL åŸå§‹è®°å½•ï¼ˆå·²å» raw_model_outputï¼‰", expanded=False):
        st.code(prettify_json(jsonl_rec, drop_keys=["raw_model_output"]), language="json")


# ============================ Jump by dataset_index ============================

if goto_ds_index.strip():
    try:
        target_idx = int(goto_ds_index.strip())
        rec = st.session_state.jsonl_idx2rec.get(target_idx, None)
        if rec is None:
            st.error(f"æœªåœ¨ JSONL ä¸­æ‰¾åˆ° dataset_index={target_idx} çš„è®°å½•ã€‚")
        else:
            key = make_composite_key(rec)
            if key not in st.session_state.parquet_key2idx:
                st.warning("å·²æ‰¾åˆ° JSONL è®°å½•ï¼Œä½†åœ¨ Parquet ä¸­æœªæ‰¾åˆ°åŒé”®è®°å½•ï¼ˆå¤åˆé”®ä¸åŒ¹é…ï¼‰ã€‚")
            render_pair_by_key(key)
        st.stop()
    except ValueError:
        st.error("dataset_index è¯·è¾“å…¥æ•´æ•°ã€‚")
        st.stop()


# ============================ Paged Browse ============================

keys = st.session_state.filtered_keys or st.session_state.matched_keys or []
total = len(keys)
if total == 0:
    st.warning("å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æ— é…å¯¹è®°å½•ã€‚è¯·å°è¯•æ¸…ç©ºè¿‡æ»¤æ¡ä»¶æˆ–æ£€æŸ¥è·¯å¾„&é”®æ˜¯å¦ä¸€è‡´ã€‚")
    st.stop()

total_pages = max(1, math.ceil(total / rows_per_page))
page_num = st.sidebar.number_input("é¡µç ", min_value=1, max_value=total_pages, value=1, step=1)

start = (page_num - 1) * rows_per_page
end = min(start + rows_per_page, total)
show_keys = keys[start:end]

st.markdown(f"### æ˜¾ç¤ºé…å¯¹è®°å½• {start}â€“{end-1} / å…± {total} æ¡ï¼ˆç¬¬ {page_num}/{total_pages} é¡µï¼‰")
if filter_aid:
    st.caption(f"è¿‡æ»¤æ¡ä»¶ï¼šarticle_accession_id = `{filter_aid}`")

for k in show_keys:
    render_pair_by_key(k)

st.caption("Med-VLM metadata viewer â€¢ Streamlit")
